# 评测数据集生成挑战赛（GPUCodeForces）

欢迎参加 **评测数据集生成挑战赛** 📊！  
本比赛旨在构建一个标准化、可用于 GPU 性能测试的评测数据集，帮助开发者更高效地比较不同硬件和框架的性能表现。

---

## 🧠 比赛背景简介

在 AI 模型开发和部署中，**GPU 性能评测**是一个非常重要的环节。  
不同 GPU、不同深度学习框架（如 PyTorch、TensorFlow、PaddlePaddle 等）在运行相同任务时，速度、吞吐量、内存占用等表现差异很大。  
本次挑战赛希望通过社区的力量，**构建一个标准化、带权重的评测数据集**，让 GPU 性能比较更加科学、公正。

---

## 🎯 比赛目标

- 从 **PyTorch、PaddlePaddle、TensorFlow、Jax、MMCV、Transformers** 等框架中收集并生成评测样本。
- 为每个样本提供**标准输出**和**性能指标**，确保结果可复现。
- 最终形成 **GPU CodeForces** 数据集和评价方法。

---

## 📥 如何参与提交？

### ✅ 参赛资格
- 你提交的PR样本中，**至少有 1 个样本被评审通过并正式整合到“GPU CodeForces” 数据集**，即可算作有效参赛。

### 📦 提交PR内容
- **一个PR包含样本的目录** [提交样例](https://gitee.com/ccf-ai-infra/GPUCodeForces/tree/main/example/001-example)
- 每个提交目录建议包含如下：
  1. **示例代码：** torch代码示例
  2. **对比代码：** 和torch对应的CUDA代码
  3. **测试代码入口：** run_code.py（请务必用这个名称，提交的PR会根据这个名称在GPU上测试结果）
  4. **其它文件（或目录）：** prompt（利用LLM从torch代码生成cuda代码的prompt示例）或者其它优化代码
  4. **PR目录说明文件：** https://gitee.com/ccf-ai-infra/GPUCodeForces/blob/main/example/001-example/readme.md

### 📦 提交PR的格式

建议在开始做题目之前创建一个赛题，提交的PR和自己创建的赛题相关联。参赛选手在每个比赛周期的目录下（例如：第一期S1、第二期S2、第三期S3...）创建一个目录，目录名称赛题的ID，例如：
```

```

---

## 🏅 竞赛排名机制

1. **优先按接受数量从高到低排序**
2. 若数量相同：
   - 比较总评分高者优先
   - 若仍相同，比加分项得分高者优先

---

## 📈 评分规则

### 📊 基础得分
| 内容 | 分值 |
|------|------|
| 提供标准 GT 输出生成函数（Numpy-CPU / 原始框架实现） | +2 分 |
| CUDA 执行时间评估 | +5 分 |
| CUDA 吞吐量评估 | +4 分 |
| CUDA 内存带宽评估 | +3 分 |

### ✨ 加分项
| 内容 | 分值 |
|------|------|
| 提供 Prompt 让 LLM 生成对应的 CUDA 代码，并同样进行性能评价 | 额外加分 |

> **接受数量** = 提交并被评审通过的样本总数

---

## 📚 术语解释

- **评测数据集**：用来测试 GPU 性能的一组标准化样本，包括代码、输入数据和预期结果。
- **GT（Ground Truth）**：标准参考答案或结果，用来验证程序运行是否正确。
- **吞吐量（Throughput）**：每秒钟能处理的数据量，越高表示 GPU 处理能力越强。
- **内存带宽（Memory Bandwidth）**：单位时间内 GPU 内存与计算核心之间的数据传输速度。
- **Prompt**：引导大语言模型（LLM）生成代码或内容的提示词。
- **LLM**：Large Language Model，大语言模型，如 ChatGPT、LLaMA 等。

---

## 📬 联系与帮助

如需更多信息或格式说明，请查看官方文档或在本仓库提交 Issue 进行讨论。  
祝你挑战成功，贡献出高质量的 GPU 评测数据集！🚀
